ERI Commons - Tokenization Utilities
------------------------------------------

This project is for common functionality relating to tokenization of textual data.
Use Lucene's tokenizers to build `TokenVectors` which can then be added to a 
`TokenVectorSet`.

`TokenVectorSet` can be used to compute TF-IDF (term frequency-inverse document 
frequency) and dot-product distances between `TokenVectors`.
